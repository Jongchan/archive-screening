### papers to read
- [Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks](https://arxiv.org/abs/1703.03400)
  - Learn features for transfer learning
- [Learning to learn by gradient descent by gradient descent](https://arxiv.org/abs/1606.04474)
  - Learn how to optimize with LSTM optimizer, compared with ADAM / RMSProp / SGD / NAG
- [Dynamic Routing with Capsules](https://arxiv.org/abs/1710.09829)
  - Progressively activate next(higher level) capsules during inference. How is it related to attention mechanisms?
- [Feature-wise transformations](https://distill.pub/2018/feature-wise-transformations/)
  - A summary of attention mechanisms in distill.pub.
